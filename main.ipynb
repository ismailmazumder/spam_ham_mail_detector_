{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"final_test.csv\")\n",
    "# null\n",
    "data = data.drop_duplicates()\n",
    "data.duplicated().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enron methanol ; meter # : 988291</td>\n",
       "      <td>\\nthis is a follow up to the note i gave you o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hpl nom for january 9 , 2001</td>\n",
       "      <td>\\n( see attached file : hplnol 09 . xls )\\n- h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neon retreat</td>\n",
       "      <td>\\nho ho ho , we ' re around to that most wonde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>photoshop , windows , office . cheap . main t...</td>\n",
       "      <td>\\nabasements darer prudently fortuitous underg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>re : indian springs</td>\n",
       "      <td>\\nthis deal is to book the teco pvr revenue . ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>fw : crosstex energy , driscoll ranch # 1 , #...</td>\n",
       "      <td>\\n9868\\nplease note the following for april pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>put the 10 on the ft</td>\n",
       "      <td>\\nthe transport volumes decreased from 25000 t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>3 / 4 / 2000 and following noms</td>\n",
       "      <td>\\nhpl can ' t take the extra 15 mmcf / d over ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>industrial worksheets for august 2000 activity</td>\n",
       "      <td>\\nattached are the worksheets for august 2000 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>important online banking alert</td>\n",
       "      <td>\\ndear valued citizensr bank member ,\\ndue to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4993 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                subject  \\\n",
       "0                     enron methanol ; meter # : 988291   \n",
       "1                          hpl nom for january 9 , 2001   \n",
       "2                                          neon retreat   \n",
       "3      photoshop , windows , office . cheap . main t...   \n",
       "4                                   re : indian springs   \n",
       "...                                                 ...   \n",
       "5165   fw : crosstex energy , driscoll ranch # 1 , #...   \n",
       "5166                               put the 10 on the ft   \n",
       "5167                    3 / 4 / 2000 and following noms   \n",
       "5169     industrial worksheets for august 2000 activity   \n",
       "5170                     important online banking alert   \n",
       "\n",
       "                                                   text  label  \n",
       "0     \\nthis is a follow up to the note i gave you o...      0  \n",
       "1     \\n( see attached file : hplnol 09 . xls )\\n- h...      0  \n",
       "2     \\nho ho ho , we ' re around to that most wonde...      0  \n",
       "3     \\nabasements darer prudently fortuitous underg...      1  \n",
       "4     \\nthis deal is to book the teco pvr revenue . ...      0  \n",
       "...                                                 ...    ...  \n",
       "5165  \\n9868\\nplease note the following for april pr...      0  \n",
       "5166  \\nthe transport volumes decreased from 25000 t...      0  \n",
       "5167  \\nhpl can ' t take the extra 15 mmcf / d over ...      0  \n",
       "5169  \\nattached are the worksheets for august 2000 ...      0  \n",
       "5170  \\ndear valued citizensr bank member ,\\ndue to ...      1  \n",
       "\n",
       "[4993 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st e word tokenize\n",
    "##### puncuation remove\n",
    "#### remove special char\n",
    "#### remove stop word\n",
    "#### Lemmatization\n",
    "#### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [this, is, a, follow, up, to, the, note, i, ga...\n",
       "1       [(, see, attached, file, :, hplnol, 09, ., xls...\n",
       "2       [ho, ho, ho, ,, we, ', re, around, to, that, m...\n",
       "3       [abasements, darer, prudently, fortuitous, und...\n",
       "4       [this, deal, is, to, book, the, teco, pvr, rev...\n",
       "                              ...                        \n",
       "5165    [9868, please, note, the, following, for, apri...\n",
       "5166    [the, transport, volumes, decreased, from, 250...\n",
       "5167    [hpl, can, ', t, take, the, extra, 15, mmcf, /...\n",
       "5169    [attached, are, the, worksheets, for, august, ...\n",
       "5170    [dear, valued, citizensr, bank, member, ,, due...\n",
       "Name: text, Length: 4993, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "data['text'] = data['text'].fillna('') # nan(not a number) also kind of NONE type thing. ekhane amra oi value gula change kore dibo\n",
    "data['text'].apply(lambda x: word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       \\nthis is a follow up to the note i gave you o...\n",
       "1       \\n see attached file  hplnol 09  xls \\n hplnol...\n",
       "2       \\nho ho ho  we  re around to that most wonderf...\n",
       "3       \\nabasements darer prudently fortuitous underg...\n",
       "4       \\nthis deal is to book the teco pvr revenue  i...\n",
       "                              ...                        \n",
       "5165    \\n9868\\nplease note the following for april pr...\n",
       "5166    \\nthe transport volumes decreased from 25000 t...\n",
       "5167    \\nhpl can  t take the extra 15 mmcf  d over th...\n",
       "5169    \\nattached are the worksheets for august 2000 ...\n",
       "5170    \\ndear valued citizensr bank member \\ndue to c...\n",
       "Name: text, Length: 4993, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punc\n",
    "import string\n",
    "def remove_punc(text):\n",
    "  trans = str.maketrans('', '', string.punctuation)\n",
    "  return text.translate(trans)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_punc)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        this is a follow up to the note i gave you on...\n",
       "1         see attached file  hplnol     xls   hplnol  ...\n",
       "2        ho ho ho  we  re around to that most wonderfu...\n",
       "3        abasements darer prudently fortuitous undergo...\n",
       "4        this deal is to book the teco pvr revenue  it...\n",
       "                              ...                        \n",
       "5165          please note the following for april prod...\n",
       "5166     the transport volumes decreased from       to...\n",
       "5167     hpl can  t take the extra    mmcf  d over the...\n",
       "5169     attached are the worksheets for august      a...\n",
       "5170     dear valued citizensr bank member  due to con...\n",
       "Name: text, Length: 4993, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### remove special char (abcd er baire habijabi)\n",
    "import re\n",
    "def remove_noise(text):\n",
    "  t = re.sub('[^a-zA-Z]', ' ', text)\n",
    "  return t\n",
    "data['text'] = data['text'].apply(remove_noise)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\im087\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       follow note gave monday preliminary flow data ...\n",
       "1                 see attached file hplnol xls hplnol xls\n",
       "2       ho ho ho around wonderful time year neon leade...\n",
       "3       abasements darer prudently fortuitous undergon...\n",
       "4       deal book teco pvr revenue understanding teco ...\n",
       "                              ...                        \n",
       "5165    please note following april production regardi...\n",
       "5166    transport volumes decreased contract thanks am...\n",
       "5167    hpl take extra mmcf weekend try next week nom ...\n",
       "5169    attached worksheets august activity three diff...\n",
       "5170    dear valued citizensr bank member due concerns...\n",
       "Name: text, Length: 4993, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### remove stop word\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "def remove_sws(text):\n",
    "    # low char\n",
    "    text = text.lower()\n",
    "    non_stop_word = []\n",
    "    split_text = text.split()\n",
    "    for new in split_text:\n",
    "        if new not in sw:\n",
    "            non_stop_word.append(new)\n",
    "            # from nltk.stem import WordNetLemmatizer\n",
    "            # lemmatizer = WordNetLemmatizer()\n",
    "            # new = lemmatizer.lemmatize(new)\n",
    "            # non_stop_word.append(new)\n",
    "    return \" \".join(non_stop_word)\n",
    "            \n",
    "# def remove_sws(text):\n",
    "#   s = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "#   return \" \".join(s)\n",
    "data['text'] = data['text'].apply(remove_sws)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\im087\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\im087\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\im087\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       follow note gave monday preliminary flow data ...\n",
       "1                   see attached file hplnol xl hplnol xl\n",
       "2       ho ho ho around wonderful time year neon leade...\n",
       "3       abasement darer prudently fortuitous undergone...\n",
       "4       deal book teco pvr revenue understanding teco ...\n",
       "                              ...                        \n",
       "5165    please note following april production regardi...\n",
       "5166    transport volume decreased contract thanks ami...\n",
       "5167    hpl take extra mmcf weekend try next week nom ...\n",
       "5169    attached worksheet august activity three diffe...\n",
       "5170    dear valued citizensr bank member due concern ...\n",
       "Name: text, Length: 4993, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lemmatization\n",
    "# similar word\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):\n",
    "    \n",
    "    lemmatize_Word = []\n",
    "    split_text = text.split()\n",
    "    for new in split_text:\n",
    "        if new not in sw:\n",
    "            lemmatize_Word.append(new)\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            new = lemmatizer.lemmatize(new)\n",
    "            lemmatize_Word.append(new)\n",
    "    return \" \".join(lemmatize_Word)\n",
    "           \n",
    "def lemma(text):\n",
    "  l = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "  return \" \".join(l)\n",
    "\n",
    "data['text'] = data['text'].apply(lemma)\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoder\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder =LabelEncoder()\n",
    "\n",
    "# data['v1']=encoder.fit_transform(data['v1'])\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Accuracy: 94.09%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       732\n",
      "           1       0.89      0.90      0.89       267\n",
      "\n",
      "    accuracy                           0.94       999\n",
      "   macro avg       0.92      0.93      0.92       999\n",
      "weighted avg       0.94      0.94      0.94       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(max_features=3000)\n",
    "x = tf.fit_transform(data['text']).toarray()\n",
    "y = data['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(x)\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "# Create the instance of Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "# Fit the data\n",
    "clf.fit(X_train, y_train)\n",
    "# Making prediction\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# your_sen = [\"join the meeting\"]\n",
    "# your_sen = tf.transform(your_sen).toarray()\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(y_pred)\n",
    "# print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for your sentence: Spam\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# tokeize korte hobe\n",
    "your_sen = [\"join and love\"]\n",
    "\n",
    "# Preprocess the new sentence before vectorizing\n",
    "your_sen = [preprocess_text(sen) for sen in your_sen]\n",
    "\n",
    "# Transform the new sentence using the same vectorizer\n",
    "your_sen_transformed = tf.transform(your_sen).toarray()\n",
    "#data['v2'].apply(lambda x: word_tokenize(x))\n",
    "# Predict the label for the new sentence\n",
    "your_sen_pred = clf.predict(your_sen_transformed)\n",
    "\n",
    "# Print the prediction for the new sentence\n",
    "print(\"Prediction for your sentence:\", 'Spam' if your_sen_pred[0] == 1 else 'Ham')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
